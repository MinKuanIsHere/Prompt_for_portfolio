# Prompt_for_portfolio
Using Large Language Model, MASK Language Model, BERT, FLAN-T5.

## The used two LLMs description of paper
1. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. https://doi.org/10.48550/arXiv.1810.04805
![image](https://github.com/MinKuanIsHere/Prompt_for_portfolio/assets/118680398/d443e240-55d7-4f34-b6eb-76bf062ebed6)

2. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei. (2022). Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. https://doi.org/10.48550/arXiv.2210.11416

## Used two LLMs from hugging face
1. https://huggingface.co/bert-base-uncased
2. https://huggingface.co/google/flan-t5-xxl




